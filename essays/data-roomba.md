# Data Roomba
I'd really like to see more large datasets implement some method of revisiting each datum in bite-sized portions. Facebook implement(ed?) this well for cleaning up their businesses. Data in those cases was likely scraped from various sources. The task at hand was not to find data, but to verify that it was truthful. Using the data avilable at the time, optimistically, is not a bad strategy for getting content out there and being transparent about the state of affairs, but without the "crowd-source roomba" in place, the data quickly falls over due to its disrepair. A typical use case of this is when a user arrives at a page that has information that they deem to be incorrect; this action would greatly elevate the priority of having the record verified. In another flow entirely, collect a mixture of random (monte carlo), oldest timestamp since update, and elevated records, and present those to a willing user to verify the data. Google's Captcha started forcing users into this kind of data mining to better their transcription and mapping services where a computer could not reliably identify words, storefronts, and various other things by way of image analysis alone. 
